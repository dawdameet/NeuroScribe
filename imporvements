V.0.2.0
âœ… Batch Normalization: Helps stabilize training and improve convergence.
âœ… LeakyReLU: Prevents neurons from becoming inactive (dying ReLU problem).
âœ… Dropout Increased: Helps prevent overfitting.
âœ… Early Stopping: Stops training if validation loss stops improving.
âœ… Model Saving: Saves the best model automatically.
âœ… More Flexibility: Now supports different activation functions, optimizers, and dropout values in YAML.
This will train faster, generalize better, and reduce overfitting. ðŸš€ðŸ”¥

V.0.3.0
âœ… Xavier/He Weight Initialization for better training.
âœ… ReduceLROnPlateau to dynamically adjust learning rate.
âœ… Gradient Clipping to prevent exploding gradients.
âœ… AdamW Optimizer with Weight Decay for better regularization.
âœ… Dataset Switching (MNIST, CIFAR-10, etc.).
âœ… Augmented Data for CIFAR-10 (rotation, flipping).
âœ… Logging & Debugging Improvements.

This setup will train faster, generalize better, and reduce overfitting significantly! ðŸš€ðŸ”¥ 

V1.0.0
âœ… Define any neural network model in YAML (without touching Python).
âœ… Use the model in Python AI projects (e.g., training, inference, saving/loading).
âœ… Modify models just by changing YAML (no need to rewrite Python code).


V1.0.1
âœ… Categorical columns are encoded.
âœ… Missing values are handled.
âœ… Labels are reshaped to avoid PyTorch tensor shape issues.
âœ… Validation split is correctly applied.

--------------- FOR selfsuff.yaml --------------------
âœ… No Python changes required â€“ model architecture, optimizer, loss function, and dataset completely controlled by YAML.
âœ… Switching from a regression task (house prices) to a  classification task (MNIST digits) by just modifying YAML.
âœ… Demonstrates self-sufficiency of your YAML-driven AI language.