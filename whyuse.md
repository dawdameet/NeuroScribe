### **What This Project is About**  
This project is a **configurable neural network trainer** built using **PyTorch**. Instead of manually writing the model code, it **reads a YAML file (`config.yaml`)** to define the model structure, training settings, and optimizer.  

It currently:  
âœ… Loads the **model architecture** from `config.yaml`  
âœ… Uses the **MNIST dataset** (handwritten digits) for training  
âœ… Trains a simple **feedforward neural network**  
âœ… **Validates** the modelâ€™s accuracy after each training epoch  

---

### **How It Works (Step by Step)**
1. **Reads `config.yaml`** â†’ Gets model structure (layers, activation functions, etc.).  
2. **Builds the model dynamically** â†’ Creates a `Sequential` PyTorch model from the YAML file.  
3. **Loads the MNIST dataset** â†’ Splits into **training** (80%) and **validation** (20%).  
4. **Trains the model** â†’ Runs multiple epochs, updating weights using an optimizer (like Adam).  
5. **Validates the model** â†’ Checks accuracy on unseen data after each epoch.  

---

### **What Itâ€™s Currently Doing**  
ðŸ“Œ Right now, itâ€™s training a simple neural network on the **MNIST dataset**. It has:  
ðŸ”¹ **2 fully connected layers** (Linear layers)  
ðŸ”¹ **ReLU activation** for non-linearity  
ðŸ”¹ **Dropout** to prevent overfitting  
ðŸ”¹ **Adam optimizer** with a learning rate of `0.001`  
ðŸ”¹ **CrossEntropyLoss** for multi-class classification  

After training, the model prints the **validation accuracy** to see how well it learned.  

**Want to Improve It?**  
- Add more layers or different activation functions in `config.yaml`  
- Change the dataset (e.g., CIFAR-10 for images)  
- Save the trained model for later use  

This project is a **beginner-friendly neural network trainer** that can be modified easily! ðŸš€ðŸ”¥